# -*- coding: utf-8 -*-
"""Copy of Delivery_Demand_Prediction_EDA_Capstone.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F464RN7vL2wVXyBbfi3aLE_FnEmMK8yC

Summary

The food delivery time prediction model plays a crucial role in the food delivery industry, where prompt and accurate delivery is of utmost importance. Delivery time directly impacts customer satisfaction and influences their overall experience.

To develop an effective prediction model, we began by thoroughly cleaning the dataset, ensuring it was free from errors and inconsistencies. This step was vital in ensuring the reliability and accuracy of the model's predictions.

Feature engineering was then employed to extract valuable insights from the dataset. By considering factors such as delivery person age, ratings, location coordinates, and time-related variables, we aimed to capture key variables that influence delivery time. These engineered features contributed to the model's predictive capabilities.

Using regression algorithms like linear regression, decision tree, random forest,XGBoost we built the predictive model. It was trained on a subset of the dataset using techniques like cross-validation to ensure robustness. Evaluation metrics such as mean squared error (MSE) and R-squared (R2) score were used to assess the model's accuracy. The food delivery time prediction model empowers businesses to optimize their operations and improve the overall delivery experience for their customers.
"""
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as stats
import statistics
import geopy
from geopy.distance import geodesic

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import OneHotEncoder
import mlflow
mlflow.set_tracking_uri("http://localhost:5000/")


from sklearn.model_selection import train_test_split,cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder,StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb
from sklearn.metrics import mean_squared_error, r2_score,mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

"""Load and Understand the data"""

df_train = pd.read_csv('train.csv')
df_train.tail()

df_train.columns

print("Train Dataset :", df_train.shape)

df_train.info()

#Check statistical values for fields with numerical datatype
df_train.describe().T

#Check statistical values for fields with other than numerical datatype
df_train.describe(exclude=np.number).T

#checking for null values

df_train.columns[df_train.isnull().any()].tolist()

"""Observations:

- Data Formatting will be required for Weatherconditions & Time_taken(min) column.

- Both numerical & categorical features are present.
- ID & Delivery_person_ID will not be used to build the model.
- Missing values are seen as 'NaN' string.
"""

#Explore each column
for column in df_train.columns:
    print(column)
    print(df_train[column].value_counts())
    print("------------------------------------")

"""Data Cleaning"""

#Update Column Names
def update_column_name(df):
    #Renaming Weatherconditions column
    df.rename(columns={'Weatherconditions': 'Weather_conditions'},inplace=True)
    df.rename(columns={'Time_taken(min)': 'Time_taken_in_min'},inplace=True)
    df.columns = [x.lower() for x in df.columns]


update_column_name(df_train)
print(df_train.columns)

df_train['delivery_person_id'].value_counts()

#Extract relevant values from column
def extract_column_value(df):
    #Extract time and convert to int
    df['time_taken_in_min'] = df['time_taken_in_min'].apply(lambda x: int(x.split(' ')[1].strip()))
    #Extract Weather conditions
    df['weather_conditions'] = df['weather_conditions'].apply(lambda x: x.split(' ')[1].strip())
    #Extract city code from Delivery person ID
    df['city_code']=df['delivery_person_id'].str.split("RES", expand=True)[0]

extract_column_value(df_train)
df_train[['time_taken_in_min','weather_conditions','city_code']].head()

df_train['time_taken_in_min']

#Check for Duplicate Values
if (len(df_train[df_train.duplicated()])>0):
    print("There are Duplicate values present")
else:
    print("There is no duplicate value present")

#Update datatypes
def update_datatype(df):
    df['delivery_person_age'] = df['delivery_person_age'].astype('float64')
    df['delivery_person_ratings'] = df['delivery_person_ratings'].astype('float64')
    df['multiple_deliveries'] = df['multiple_deliveries'].astype('float64')
    df['order_date']=pd.to_datetime(df['order_date'],format="%d-%m-%Y")

update_datatype(df_train)

#Convert String 'NaN' to np.nan
def convert_nan(df):
    df.replace('NaN', float(np.nan), regex=True,inplace=True)
    df.replace('nan', float(np.nan), regex=True,inplace=True)
convert_nan(df_train)

#Check null values
df_train.isnull().sum().sort_values(ascending=False)


#Handle null values

# For categorical columns use mode as imputation for np.nan
## ["weatherconditions","road_traffic_density","city_area","festival"]

# For numeric columns use median or mode as imputation for np.nan
## ['delivery_person_age', 'delivery_person_ratings'] -- median
## ['multiple_deliveries'] -- mode

#

def handle_null_values(df):
    df["delivery_person_age"] = df["delivery_person_age"].fillna(df["delivery_person_age"].astype("float").median())
    df['weather_conditions'].fillna(df['weather_conditions'].mode()[0], inplace=True)
    df['city'].fillna(df['city'].mode()[0], inplace=True)
    df['festival'].fillna(df['festival'].mode()[0], inplace=True)
    df['multiple_deliveries'].fillna(df['multiple_deliveries'].mode()[0], inplace=True)
    df['road_traffic_density'].fillna(df['road_traffic_density'].mode()[0], inplace=True)     # ====> Sub problem explore another model for prediction
    df['delivery_person_ratings'].fillna(df['delivery_person_ratings'].median(), inplace=True)
    df["delivery_person_ratings"] = df["delivery_person_ratings"].replace(6, 5)
    df.replace(np.nan,np.random.choice(df['delivery_person_age']) , regex=True,inplace=True)

handle_null_values(df_train)
df_train.isnull().sum()

df_train.isnull().sum()

# Calculate percentage for each category
city_area = df_train['city'].value_counts()
labels = city_area.index
sizes = city_area.values

# Calculate percentage for each category
city_code = df_train['city_code'].value_counts()
labels = city_code.index
sizes = city_code.values

# Calculate percentage for each category
festival_code = df_train['festival'].value_counts()
labels = festival_code.index
sizes = festival_code.values

#Drop Columns which won't be use for building model
def drop_columns(df):
    df.drop(['id','delivery_person_id'],axis=1,inplace=True)

print("Before No. of columns: ",df_train.shape[1])
drop_columns(df_train)
print("After No. of columns: ",df_train.shape[1])

"""#Vizualizations"""

import numpy as np
import pandas as pd

# For plotting maps
import folium

# For Regular Expressions
import re

# For working with geographical data
import geopandas

# For plotting in python
import matplotlib
import matplotlib.pyplot as plt
df_indore_data = df_train[df_train['city_code']=='INDO']

df_indore_data.info()

m = folium.Map([22.7196, 75.8577], zoom_start=11)

for _, row in df_indore_data.head(50).iterrows():
    folium.CircleMarker([row['restaurant_latitude'], row['restaurant_longitude']],
                        radius=15,
                        fill_color="#3db7e4", # divvy color
                       ).add_to(m)

    folium.CircleMarker([row['delivery_location_latitude'], row['delivery_location_longitude']],
                        radius=15,
                        fill_color="red", # divvy color
                       ).add_to(m)

    folium.PolyLine([[row['restaurant_latitude'], row['restaurant_longitude']],
                     [row['delivery_location_latitude'], row['delivery_location_longitude']]]).add_to(m)
m

"""# Feature Engineering"""

def extract_date_features(data):
    data["day"] = data.order_date.dt.day
    data["month"] = data.order_date.dt.month
    data["quarter"] = data.order_date.dt.quarter
    data["year"] = data.order_date.dt.year
    data['day_of_week'] = data.order_date.dt.day_of_week.astype(int)
    data["is_month_start"] = data.order_date.dt.is_month_start.astype(int)
    data["is_month_end"] = data.order_date.dt.is_month_end.astype(int)
    data["is_quarter_start"] = data.order_date.dt.is_quarter_start.astype(int)
    data["is_quarter_end"] = data.order_date.dt.is_quarter_end.astype(int)
    data["is_year_start"] = data.order_date.dt.is_year_start.astype(int)
    data["is_year_end"] = data.order_date.dt.is_year_end.astype(int)
    data['is_weekend'] = np.where(data['day_of_week'].isin([5,6]),1,0)

extract_date_features(df_train)
df_train.head()

#Calculate Time Difference

def calculate_picked_time(row):
    # Add a day offset if 'time_order_picked' is less than 'time_orderd'
    if row['time_order_picked'] < row['time_orderd']:
        return row['order_date'] + pd.DateOffset(days=1) + row['time_order_picked']
    else:
        return row['order_date'] + row['time_order_picked']

def calculate_time_diff(df):
    # Find the difference between ordered time & picked time
    df['time_orderd'] = pd.to_timedelta(df['time_orderd'])
    df['time_order_picked'] = pd.to_timedelta(df['time_order_picked'])

    df['time_order_picked_formatted'] = df.apply(calculate_picked_time, axis=1)
    df['time_ordered_formatted'] = df['order_date'] + df['time_orderd']

    df['time_order_picked_formatted'] = pd.to_datetime(df['time_order_picked_formatted'])
    df['time_ordered_formatted'] = pd.to_datetime(df['time_ordered_formatted'])

    df['order_prepare_time'] = (df['time_order_picked_formatted'] - df['time_ordered_formatted']).dt.total_seconds() / 60

    # Handle null values by filling with the median
    df['order_prepare_time'].fillna(df['order_prepare_time'].median(), inplace=True)

    # Drop all the time & date related columns
    df.drop(['time_orderd', 'time_order_picked', 'time_ordered_formatted', 'time_order_picked_formatted', 'order_date'], axis=1, inplace=True)


calculate_time_diff(df_train)
df_train.head()

#Calculate distance between restaurant location & delivery location

# using haversine method
def haversine_distance(loc_list):
    # earth's radius in km
    R = 6371.0

    # convert lat and lon from deg to radians
    lat1,lon1,lat2,lon2 = map(np.radians,loc_list)
    # diff between lat and lon
    d_lat = lat2 - lat1
    d_lon = lon2 - lon1
    # applying haversine formula
    a = np.sin(d_lat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(d_lon / 2)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))
    d = R * c
    return round(d,2)

loc_cols = ["restaurant_latitude", "restaurant_longitude", "delivery_location_latitude", "delivery_location_longitude"]

df_train['restaurant_latitude'] = pd.to_numeric(df_train['restaurant_latitude'], errors='coerce')
df_train['restaurant_longitude'] = pd.to_numeric(df_train['restaurant_longitude'], errors='coerce')
df_train['delivery_location_latitude'] = pd.to_numeric(df_train['delivery_location_latitude'], errors='coerce')
df_train['delivery_location_longitude'] = pd.to_numeric(df_train['delivery_location_longitude'], errors='coerce')

distance = []

    # Iterate over each row in the DataFrame
for _, row in df_train.iterrows():
    # Extract values for the location columns
    location_list = [row[col] for col in loc_cols]

    # Calculate haversine distance
    distance.append(haversine_distance(location_list))

# Add the distances as a new column
df_train["distance"] = distance


df_train.head()

# Issue while doing predction as it expects str
#df_train[['restaurant_latitude', 'restaurant_longitude', 'delivery_location_latitude', 'delivery_location_longitude']] = df_train[['restaurant_latitude', 'restaurant_longitude', 'delivery_location_latitude', 'delivery_location_longitude']].astype(str)

# Strip spaces from the object type  columns as there is space for NaN
df_train = df_train.apply(lambda x: x.str.strip() if x.dtype == "object" else x)

# To remove the rows with zero lat and long
def drop_zero_lat_long(data):
    dataframe = data[-((data["restaurant_latitude"]==0.0) & (data["restaurant_longitude"]==0.0)) ]
    return dataframe


df_train = drop_zero_lat_long(df_train)

"""### Label Encoding - Categorical Features"""

weather_mappings = {
    "Sunny": 0,
    "Cloudy": 1,
    "Fog": 2,
    "Windy": 3,
    "Stormy": 4,
    "Sandstorms": 5,
}

traff_den_mappings = {
    "Low": 0,
    "Medium": 1,
    "High": 2,
    "Jam": 3,
}

order_type_mappings = {
    "Snack": 0,
    "Drinks": 1,
    "Buffet": 2,
    "Meal": 3
}

vehicle_mappings = {
    "bicycle": 0,
    "scooter": 1,
    "electric_scooter": 2,
    "motorcycle": 3,
}

festival_mappings = {
    "No": 0,
    "Yes": 1
}

city_area_mappings = {
    "Semi-Urban": 0,
    "Urban": 1,
    "Metropolitian": 2,
}

city_mappings = {
    "INDO": 0,
    "BANG": 1,
    "COIMB": 2,
    "CHEN": 3,
    "HYD": 4,
    "RANCHI": 5,
    "MYS": 6,
    "DEH": 7,
    "KOC": 8,
    "PUNE": 9,
    "LUDH": 10,
    "KNP": 11,
    "MUM": 12,
    "KOL": 13,
    "JAP": 14,
    "SUR": 15,
    "GOA": 16,
    "AURG": 17,
    "AGR": 18,
    "VAD": 19,
    "ALH": 20,
    "BHP": 21
}

mnth_mappings = {
    "January": 0,
    "February": 1,
    "March": 2,
    "April": 3,
    "May": 4,
    "June": 5,
    "July": 6,
    "August": 7,
    "September": 8,
    "October": 9,
    "November": 10,
    "December": 11
}

class WeekdayImputer(BaseEstimator, TransformerMixin):
    """ Impute missing values in 'weekday' column by extracting dayname from 'dteday' column """

    def __init__(self, variable: str, date_var:str):

        if not isinstance(variable, str):
            raise ValueError("variable name should be a string")
        if not isinstance(date_var, str):
            raise ValueError("date variable name should be a string")

        self.variable = variable
        self.date_var = date_var

    def fit(self, X: pd.DataFrame, y: pd.Series = None):
        # we need the fit statement to accomodate the sklearn pipeline
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = X.copy()
        # convert 'dteday' column to Datetime datatype
        X[self.date_var] = pd.to_datetime(X[self.date_var], format='%Y-%m-%d')

        wkday_null_idx = X[X[self.variable].isnull() == True].index
        X.loc[wkday_null_idx, self.variable] = X.loc[wkday_null_idx, self.date_var].dt.day_name().apply(lambda x: x[:3])

        # drop 'dteday' column after imputation
        X.drop(self.date_var, axis=1, inplace=True)

        return X


class WeathersitImputer(BaseEstimator, TransformerMixin):
    """ Impute missing values in 'weathersit' column by replacing them with the most frequent category value """

    def __init__(self, variable: str):

        if not isinstance(variable, str):
            raise ValueError("variable name should be a string")

        self.variable = variable

    def fit(self, X: pd.DataFrame, y: pd.Series = None):
        # we need the fit statement to accomodate the sklearn pipeline
        X = X.copy()
        self.fill_value = X[self.variable].mode()[0]

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = X.copy()
        X[self.variable] = X[self.variable].fillna(self.fill_value)

        return X


class Mapper(BaseEstimator, TransformerMixin):
    """
    Ordinal categorical variable mapper:
    Treat column as Ordinal categorical variable, and assign values accordingly
    """

    def __init__(self, variable:str, mappings:dict):

        if not isinstance(variable, str):
            raise ValueError("variable name should be a string")

        self.variable = variable
        self.mappings = mappings

    def fit(self, X: pd.DataFrame, y: pd.Series = None):
        # we need the fit statement to accomodate the sklearn pipeline
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = X.copy()
        X[self.variable] = X[self.variable].map(self.mappings).astype(int)

        return X


class OutlierHandler(BaseEstimator, TransformerMixin):
    """
    Change the outlier values:
        - to upper-bound, if the value is higher than upper-bound, or
        - to lower-bound, if the value is lower than lower-bound respectively.
    """

    def __init__(self, variable:str):

        if not isinstance(variable, str):
            raise ValueError("variable name should be a string")

        self.variable = variable

    def fit(self, X: pd.DataFrame, y: pd.Series = None):
        # we need the fit statement to accomodate the sklearn pipeline
        X = X.copy()
        q1 = X.describe()[self.variable].loc['25%']
        q3 = X.describe()[self.variable].loc['75%']
        iqr = q3 - q1
        self.lower_bound = q1 - (1.5 * iqr)
        self.upper_bound = q3 + (1.5 * iqr)

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = X.copy()

        for i in X.index:
            if X.loc[i, self.variable] > self.upper_bound:
                X.loc[i, self.variable]= self.upper_bound
            if X.loc[i, self.variable] < self.lower_bound:
                X.loc[i, self.variable]= self.lower_bound

        return X


class WeekdayOneHotEncoder(BaseEstimator, TransformerMixin):
    """ One-hot encode weekday column """

    def __init__(self, variable:str):

        if not isinstance(variable, str):
            raise ValueError("variable name should be a string")

        self.variable = variable
        self.encoder = OneHotEncoder(sparse_output=False)

    def fit(self, X: pd.DataFrame, y: pd.Series = None):
        # we need the fit statement to accomodate the sklearn pipeline
        X = X.copy()
        self.encoder.fit(X[[self.variable]])
        # Get encoded feature names
        self.encoded_features_names = self.encoder.get_feature_names_out([self.variable])

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = X.copy()

        encoded_weekdays = self.encoder.transform(X[[self.variable]])
        # Append encoded weekday features to X
        X[self.encoded_features_names] = encoded_weekdays

        # drop 'weekday' column after encoding
        X.drop(self.variable, axis=1, inplace=True)

        return X

df_train.describe() ## feature scaling is required

pd.set_option('display.max_columns',50)
df_train.head(2)

plt.figure(figsize=(20,7))
sns.heatmap(df_train[['delivery_person_age', 'delivery_person_ratings','vehicle_condition', 'multiple_deliveries',
       'time_taken_in_min', 'day', 'month', 'quarter', 'year',
       'day_of_week', 'is_month_start', 'is_month_end', 'is_quarter_start',
       'is_quarter_end', 'is_year_start', 'is_year_end', 'is_weekend',
       'order_prepare_time', 'distance']].corr(), annot=True)

df_train.columns

prep_data= df_train.copy()

prep_data["weather_conditions"] = prep_data["weather_conditions"].map(weather_mappings)
prep_data["road_traffic_density"] = prep_data["road_traffic_density"].map(traff_den_mappings)
prep_data["type_of_order"] = prep_data["type_of_order"].map(order_type_mappings)
prep_data["type_of_vehicle"] = prep_data["type_of_vehicle"].map(vehicle_mappings)
prep_data["city"] = prep_data["city"].map(city_area_mappings)
prep_data["festival"] = prep_data["festival"].map(festival_mappings)
prep_data["month"] = prep_data["month"].map(mnth_mappings)

prep_data.isna().sum()

prep_data.shape

prep_data.describe()

# prep_data[['distance', 'order_prepare_time']
prep_data[prep_data['distance']>30]

prep_data['distance'].mean()

prep_data['distance'].median()

prep_data['distance'] = np.where(prep_data['distance']>30,prep_data['distance'].median(),prep_data['distance'])

print(prep_data['order_prepare_time'].mean())
print(prep_data['order_prepare_time'].median())

# prep_data[['distance', 'order_prepare_time']
prep_data[prep_data['order_prepare_time']>50]

prep_data['order_prepare_time'] = np.where(prep_data['order_prepare_time']>60,prep_data['order_prepare_time'].median(),prep_data['order_prepare_time'])

prep_data.head()

target = ['time_taken_in_min']
features = ['delivery_person_age', 'delivery_person_ratings','weather_conditions', 'road_traffic_density', 'vehicle_condition', 'type_of_order',
            'type_of_vehicle', 'multiple_deliveries', 'festival', 'city', 'day_of_week', 'is_weekend', 'is_month_start',	'is_month_end',
            'quarter', 'distance', 'order_prepare_time']

prep_data.describe()

# divide train and test
X_train, X_test, y_train, y_test = train_test_split(

        prep_data[features],     # predictors
        prep_data[target],       # target
        test_size = 0.2,
        random_state= 42,   # set the random seed here for reproducibility
    )
# split across date - strategy

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import GridSearchCV, cross_val_score, RandomizedSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
import xgboost as xgb
# from catboost import CatBoostRegressor
import optuna

from sklearn.metrics import mean_squared_error, r2_score, mean_squared_error, mean_absolute_percentage_error

X_train.head(2)

client = mlflow.tracking.MlflowClient()

# experiment_name = "DeliveryTimePrediction-EDA"

# Create or set the experiment
# exp = mlflow.set_experiment(experiment_name)
exp = mlflow.set_experiment(experiment_name = "Driver-Delivery-Time-Prediction-New")

    # Start an MLflow run

# mlflow.start_run(experiment_id= exp.experiment_id, run_name= "LinearRegression")

"""## Linear Regression"""
"""
linear_reg = LinearRegression()
linear_reg.fit(X_train, y_train)
y_pred = linear_reg.predict(X_test)
print("R2 score:", r2_score(y_test, y_pred))
print("Mean squared error:", mean_squared_error(y_test, y_pred))
print(f"Root mean squared error:{np.sqrt(mean_squared_error(y_test,y_pred))}")
print("Mean Absolute Pecentage error:",mean_absolute_percentage_error(y_test,y_pred))

mlflow.log_param("best_param", "")
mlflow.log_metric("r2_score", r2_score(y_test, y_pred))
mlflow.log_metric("rmse", np.sqrt(mean_squared_error(y_test,y_pred)))
mlflow.log_metric("mse", mean_squared_error(y_test, y_pred))
mlflow.log_metric("mape", np.sqrt(mean_absolute_percentage_error(y_test,y_pred)))
# End an active MLflow run
mlflow.end_run()

## Random Forest Regressor
mlflow.start_run(experiment_id= exp.experiment_id, run_name= "RandomForestRegressor")
random_ft =  RandomForestRegressor()
random_ft.fit(X_train, y_train)
y_pred_rf = random_ft.predict(X_test)
print("R2 score:", r2_score(y_test, y_pred_rf))
print("Mean squared error:", mean_squared_error(y_test, y_pred_rf))
print(f"Root mean squared error:{np.sqrt(mean_squared_error(y_test,y_pred_rf))}")
print("Mean Absolute Pecentage error:",mean_absolute_percentage_error(y_test,y_pred_rf))

mlflow.log_param("best_param", "")
mlflow.log_metric("r2_score", r2_score(y_test, y_pred_rf))
mlflow.log_metric("rmse", np.sqrt(mean_squared_error(y_test,y_pred_rf)))
mlflow.log_metric("mse", mean_squared_error(y_test, y_pred_rf))
mlflow.log_metric("mape", np.sqrt(mean_absolute_percentage_error(y_test,y_pred_rf)))
# End an active MLflow run
mlflow.end_run()

## XGBoost Regressor

mlflow.start_run(experiment_id= exp.experiment_id, run_name= "XGBRegressor")
xgboost =  XGBRegressor()
xgboost.fit(X_train, y_train)
y_pred_xg = xgboost.predict(X_test)
print("R2 score:", r2_score(y_test, y_pred_xg))
print("Mean squared error:", mean_squared_error(y_test, y_pred_xg))
print(f"Root mean squared error:{np.sqrt(mean_squared_error(y_test,y_pred_xg))}")
print("Mean Absolute Pecentage error:",mean_absolute_percentage_error(y_test,y_pred_xg))

mlflow.log_param("best_param", "")
mlflow.log_metric("r2_score", r2_score(y_test, y_pred_xg))
mlflow.log_metric("rmse", np.sqrt(mean_squared_error(y_test,y_pred_xg)))
mlflow.log_metric("mse", mean_squared_error(y_test, y_pred_xg))
mlflow.log_metric("mape", np.sqrt(mean_absolute_percentage_error(y_test,y_pred_xg)))
# End an active MLflow run
mlflow.end_run()


## Catboost Regressor
mlflow.start_run(experiment_id= exp.experiment_id, run_name= "CatBoostRegressor")

catboost =  CatBoostRegressor()
catboost.fit(X_train, y_train, verbose=False)
y_pred_cat = catboost.predict(X_test)
print("R2 score:", r2_score(y_test, y_pred_cat))
print("Mean squared error:", mean_squared_error(y_test, y_pred_cat))
print(f"Root mean squared error:{np.sqrt(mean_squared_error(y_test,y_pred_cat))}")
print("Mean Absolute Pecentage error:",mean_absolute_percentage_error(y_test,y_pred_cat))

mlflow.log_param("best_param", "")
mlflow.log_metric("r2_score", r2_score(y_test, y_pred_cat))
mlflow.log_metric("rmse", np.sqrt(mean_squared_error(y_test,y_pred_cat)))
mlflow.log_metric("mse", mean_squared_error(y_test, y_pred_cat))
mlflow.log_metric("mape", np.sqrt(mean_absolute_percentage_error(y_test,y_pred_cat)))
# End an active MLflow run
mlflow.end_run()


## catboost regressor afte specifing the categorical features
mlflow.start_run(experiment_id= exp.experiment_id, run_name= "CatBoostRegressor-catFeat")
"""
cat_data = df_train.copy()
cat_data['distance'] = np.where(cat_data['distance']>30,cat_data['distance'].median(),cat_data['distance'])
cat_data['order_prepare_time'] = np.where(cat_data['order_prepare_time']>60,cat_data['order_prepare_time'].median(),cat_data['order_prepare_time'])

target_cat = ['time_taken_in_min']
features_cat = ['delivery_person_age', 'delivery_person_ratings','weather_conditions', 'road_traffic_density', 'vehicle_condition', 'type_of_order',
            'type_of_vehicle', 'multiple_deliveries', 'festival', 'city', 'day_of_week', 'is_weekend', 'is_month_start',	'is_month_end',
            'quarter', 'distance', 'order_prepare_time']

cat_data[features_cat].head(2)

# cat_data[['is_weekend','is_month_start','is_month_end']].describe()
col_list = ['is_weekend','is_month_start','is_month_end']
for i in col_list:
  cat_data[i] = np.where(cat_data[i]==0,'No','Yes')

cat_data[['is_weekend','is_month_start','is_month_end']].head(2)

object_cols_indices = list(cat_data.select_dtypes(include='object').columns)

cat_data

object_cols_indices =['weather_conditions',
 'road_traffic_density',
 'type_of_order',
 'type_of_vehicle',
 'festival',
 'city',
 'is_month_start',
 'is_month_end',
 'is_weekend']

# divide train and test
X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(

        cat_data[features_cat],     # predictors
        cat_data[target_cat],       # target
        test_size = 0.2,
        random_state= 42,   # set the random seed here for reproducibility
    )
"""
catboost_2 =  CatBoostRegressor()
catboost_2.fit(X_train_cat, y_train_cat, verbose=False, cat_features=object_cols_indices)
y_pred_cat = catboost_2.predict(X_test_cat)
print("R2 score:", r2_score(y_test_cat, y_pred_cat))
print("Mean squared error:", mean_squared_error(y_test_cat, y_pred_cat))
print(f"Root mean squared error:{np.sqrt(mean_squared_error(y_test_cat,y_pred_cat))}")
print("Mean Absolute Pecentage error:",mean_absolute_percentage_error(y_test_cat,y_pred_cat))

mlflow.log_param("best_param", "")
mlflow.log_metric("r2_score", r2_score(y_test, y_pred_cat))
mlflow.log_metric("rmse", np.sqrt(mean_squared_error(y_test,y_pred_cat)))
mlflow.log_metric("mse", mean_squared_error(y_test, y_pred_cat))
mlflow.log_metric("mape", np.sqrt(mean_absolute_percentage_error(y_test,y_pred_cat)))
# End an active MLflow run
mlflow.end_run()



## Gridsearch for XGboost
mlflow.start_run(experiment_id= exp.experiment_id, run_name= "XGBRegressor-RandomSearch")

param_dist = {'n_estimators': stats.randint(10, 500),
              'learning_rate': stats.uniform(0.001, 0.2),
              'subsample': stats.uniform(0.3, 0.9),
              'max_depth': [3, 4, 5, 6, 7, 8, 9],
              'colsample_bytree': stats.uniform(0.5, 0.9),
              'min_child_weight': [1, 2, 3, 4]
             }

xgb = XGBRegressor(seed=42) # Initialize GridSearchCV
randm_search = RandomizedSearchCV(estimator=xgb, param_distributions=param_dist, cv=5, scoring='neg_mean_squared_error')
# Fit the model
randm_search.fit(X_train, y_train)
# Print the best parameters
print(f'Best parameters: {randm_search.best_params_}')
# Make predictions with the best model
y_pred_rnd = randm_search.best_estimator_.predict(X_test)
# Evaluate the model
mse = mean_squared_error(y_test, y_pred_rnd)
print(r2_score(y_test, y_pred_rnd))
print(mean_squared_error(y_test, y_pred_rnd))

mlflow.log_param("best_param", randm_search.best_params_)
mlflow.log_metric("r2_score", r2_score(y_test, y_pred_rnd))
mlflow.log_metric("rmse", np.sqrt(mean_squared_error(y_test,y_pred_rnd)))
mlflow.log_metric("mse", mean_squared_error(y_test, y_pred_rnd))
mlflow.log_metric("mape", np.sqrt(mean_absolute_percentage_error(y_test,y_pred_rnd)))
# End an active MLflow run
mlflow.end_run()

"""
## Tuning Xgboost regressor using hyperopt


## Optuna model-1
exp = mlflow.set_experiment(experiment_name = "Driver-Delivery-Time-Prediction-New")

mlflow.start_run(experiment_id= exp.experiment_id, run_name= "XGBRegressor-Optuna-1")

def objective(trial):
    # Define the hyperparameter search space
    param = {
        'verbosity': 0,
        'objective': 'reg:squarederror',
        'max_depth': trial.suggest_int('max_depth', 3, 15),
        'n_estimators': trial.suggest_int('n_estimators',100, 500),
        'learning_rate': trial.suggest_float('learning_rate',0.001, 0.2),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
    }

    # Train the model
    xgb_optuna = XGBRegressor(**param)
    xgb_optuna.fit(X_train, y_train)

    # Make predictions
    preds = xgb_optuna.predict(X_test)

    # Calculate mean squared error
    mse = mean_squared_error(y_test, preds)
    return mse

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100)

# Get the best hyperparameters
best_params = study.best_params
print(f'Best hyperparameters: {best_params}')

best_params = study.best_params
print(f'Best hyperparameters: {best_params}')
"""
# model with optuna -1

"""
# xgboost_1 =  XGBRegressor(max_depth=9, n_estimators=437, learning_rate= 0.012706457541012496,
                        #   subsample=0.6931194195294401, colsample_bytree= 0.9303143908925534)
xgboost_1 =  XGBRegressor(random_state=42,**best_params)
xgboost_1.fit(X_train, y_train)
y_pred_xg_1 = xgboost_1.predict(X_test)
print("R2 score:", r2_score(y_test, y_pred_xg_1))
print("Mean squared error:", mean_squared_error(y_test, y_pred_xg_1))
print(f"Root mean squared error:{np.sqrt(mean_squared_error(y_test,y_pred_xg_1))}")
print("Mean Absolute Pecentage error:",mean_absolute_percentage_error(y_test,y_pred_xg_1))


# mlflow.log_param("best_param", best_params)
mlflow.log_param("n_estimators", best_params['n_estimators'])
mlflow.log_param("max_depth", best_params['max_depth'])
mlflow.log_param("learning_rate", best_params['learning_rate'])
mlflow.log_param("subsample", best_params['subsample'])
mlflow.log_param("colsample_bytree", best_params['colsample_bytree'])

mlflow.log_metric("r2_score", r2_score(y_test, y_pred_xg_1))
mlflow.log_metric("rmse", np.sqrt(mean_squared_error(y_test,y_pred_xg_1)))
mlflow.log_metric("mse", mean_squared_error(y_test, y_pred_xg_1))
mlflow.log_metric("mape", np.sqrt(mean_absolute_percentage_error(y_test,y_pred_xg_1)))

import mlflow.pyfunc
model_name = "delivery_time_model"         #"sklearn-titanic-rf-model"
    
# Register new model/version of model
mlflow.sklearn.log_model(sk_model = xgboost_1, 
                            artifact_path="trained_model",
                            registered_model_name=model_name
                            )
# Add 'last-trained' alias to this new model version
client.set_registered_model_alias(name=model_name, alias="last-trained", version=str(1))
client.set_registered_model_alias(name=model_name, alias="production", version=str(1))
client.set_registered_model_alias(name=model_name, alias="experiment-best-model", version=str(1))

# End an active MLflow run
mlflow.end_run()

"""

# Optuna-model-2
mlflow.start_run(experiment_id= exp.experiment_id, run_name= "XGBRegressor-Optuna-2")

def objective(trial):
    # Define the hyperparameter search space
    param = {
        'verbosity': 0,
        'objective': 'reg:squarederror',
        # 'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),
        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),
        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'eta': trial.suggest_float('eta', 1e-8, 1.0, log=True),
        'max_depth': trial.suggest_int('max_depth', 1, 9),
        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'gamma': trial.suggest_float('gamma', 0, 5)
    }

    # Train the model
    xgb_optuna = XGBRegressor(**param)
    xgb_optuna.fit(X_train, y_train)

    # Make predictions
    preds = xgb_optuna.predict(X_test)

    # Calculate mean squared error
    mse = mean_squared_error(y_test, preds)
    return mse

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100)

# Get the best hyperparameters
best_params = study.best_params
print(f'Best hyperparameters: {best_params}')

xgboost_2 =  XGBRegressor( alpha=1.5669943104632824e-08, subsample=0.7682980408655012,
                          colsample_bytree= 0.901098462236655, eta= 0.005716113095564376, max_depth= 9, n_estimators= 777,
                          min_child_weight= 2, gamma= 3.4024846596266887)
xgboost_2.fit(X_train, y_train)
y_pred_xg_2 = xgboost_2.predict(X_test)
print("R2 score:", r2_score(y_test, y_pred_xg_2))
print("Mean squared error:", mean_squared_error(y_test, y_pred_xg_2))
print(f"Root mean squared error:{np.sqrt(mean_squared_error(y_test,y_pred_xg_2))}")
print("Mean Absolute Pecentage error:",mean_absolute_percentage_error(y_test,y_pred_xg_2))


mlflow.log_param("best_param", best_params)
mlflow.log_metric("r2_score", r2_score(y_test, y_pred_xg_2))
mlflow.log_metric("rmse", np.sqrt(mean_squared_error(y_test,y_pred_xg_2)))
mlflow.log_metric("mse", mean_squared_error(y_test, y_pred_xg_2))
mlflow.log_metric("mape", np.sqrt(mean_absolute_percentage_error(y_test,y_pred_xg_2)))
# End an active MLflow run
mlflow.end_run()


## Catboost model Tuning

mlflow.start_run(experiment_id= exp.experiment_id, run_name= "CatBoostRegressor-Optuna-1")

def objective(trial):
    # Define the hyperparameter search space
    param = {
        'iterations': trial.suggest_int('iterations', 100, 1000),
        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),
        'depth': trial.suggest_int('depth', 4, 10),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 100, log=True),
        # 'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),
        # 'subsample': trial.suggest_float('subsample', 0.5, 1) if trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']) == 'Bernoulli' else 1,
        # 'random_strength': trial.suggest_float('random_strength', 1e-8, 10, log=True),
        # 'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 10) if trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']) == 'Bayesian' else 0,
        'border_count': trial.suggest_int('border_count', 1, 255),
        'max_ctr_complexity': trial.suggest_int('max_ctr_complexity', 1, 4),
        'verbose': 0
    }

    # Initialize CatBoost Regressor
    model = CatBoostRegressor(**param)

    # Train the model
    model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=100, verbose=0)

    # Make predictions
    preds = model.predict(X_test)

    # Calculate mean squared error
    mse = mean_squared_error(y_test, preds)
    return mse

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100)

# Get the best hyperparameters
best_params = study.best_params
print(f'Best hyperparameters: {best_params}')

catboost_1 =  CatBoostRegressor(iterations= 720, learning_rate=0.031585709631514906, depth= 9,
                                l2_leaf_reg=0.0003060992973179909, border_count=216,max_ctr_complexity= 1)
catboost_1.fit(X_train, y_train, verbose=False)
y_pred_cat_1 = catboost_1.predict(X_test)
print("R2 score:", r2_score(y_test, y_pred_cat_1))
print("Mean squared error:", mean_squared_error(y_test, y_pred_cat_1))
print(f"Root mean squared error:{np.sqrt(mean_squared_error(y_test,y_pred_cat_1))}")
print("Mean Absolute Pecentage error:",mean_absolute_percentage_error(y_test,y_pred_cat_1))

mlflow.log_param("best_param", best_params)
mlflow.log_metric("r2_score", r2_score(y_test, y_pred_cat_1))
mlflow.log_metric("rmse", np.sqrt(mean_squared_error(y_test,y_pred_cat_1)))
mlflow.log_metric("mse", mean_squared_error(y_test, y_pred_cat_1))
mlflow.log_metric("mape", np.sqrt(mean_absolute_percentage_error(y_test,y_pred_cat_1)))
# End an active MLflow run
mlflow.end_run()

# Optuna 2 for catboost

mlflow.start_run(experiment_id= exp.experiment_id, run_name= "CatBoostRegressor-Optuna-2")
def objective(trial):
    # Define the hyperparameter search space
    param = {
        'iterations': trial.suggest_int('iterations', 100, 1000),
        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),
        'depth': trial.suggest_int('depth', 4, 10),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 100, log=True),
        # 'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),
        # 'subsample': trial.suggest_float('subsample', 0.5, 1) if trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']) == 'Bernoulli' else 1,
        # 'random_strength': trial.suggest_float('random_strength', 1e-8, 10, log=True),
        # 'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 10) if trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']) == 'Bayesian' else 0,
        # 'border_count': trial.suggest_int('border_count', 1, 255),
        # 'max_ctr_complexity': trial.suggest_int('max_ctr_complexity', 1, 4),
        'verbose': 0
    }

    # Initialize CatBoost Regressor
    model = CatBoostRegressor(**param)

    # Train the model
    model.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=100, verbose=0)

    # Make predictions
    preds = model.predict(X_test)

    # Calculate mean squared error
    mse = mean_squared_error(y_test, preds)
    return mse

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100)

# Get the best hyperparameters
best_params = study.best_params
print(f'Best hyperparameters: {best_params}')

catboost_2 =  CatBoostRegressor(iterations= 923, learning_rate=0.018290754432461944, depth= 9,
                                l2_leaf_reg=0.018290754432461944)
catboost_2.fit(X_train, y_train, verbose=False)
y_pred_cat_2 = catboost_2.predict(X_test)
print("R2 score:", r2_score(y_test, y_pred_cat_2))
print("Mean squared error:", mean_squared_error(y_test, y_pred_cat_2))
print(f"Root mean squared error:{np.sqrt(mean_squared_error(y_test,y_pred_cat_2))}")
print("Mean Absolute Pecentage error:",mean_absolute_percentage_error(y_test,y_pred_cat_2))

mlflow.log_param("best_param", best_params)
mlflow.log_metric("r2_score", r2_score(y_test, y_pred_cat_2))
mlflow.log_metric("rmse", np.sqrt(mean_squared_error(y_test,y_pred_cat_2)))
mlflow.log_metric("mse", mean_squared_error(y_test, y_pred_cat_2))
mlflow.log_metric("mape", np.sqrt(mean_absolute_percentage_error(y_test,y_pred_cat_2)))
# End an active MLflow run
mlflow.end_run()

#####################################################

## catboost with categorical features-optuna-3
mlflow.start_run(experiment_id= exp.experiment_id, run_name= "CatBoostRegressor-catFeat-Optuna-3")

def objective(trial):
    # Define the hyperparameter search space
    param = {
        'iterations': trial.suggest_int('iterations', 100, 1000),
        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),
        'depth': trial.suggest_int('depth', 4, 10),
        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 100, log=True),
        # 'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),
        # 'subsample': trial.suggest_float('subsample', 0.5, 1) if trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']) == 'Bernoulli' else 1,
        # 'random_strength': trial.suggest_float('random_strength', 1e-8, 10, log=True),
        # 'bagging_temperature': trial.suggest_float('bagging_temperature', 0, 10) if trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']) == 'Bayesian' else 0,
        # 'border_count': trial.suggest_int('border_count', 1, 255),
        # 'max_ctr_complexity': trial.suggest_int('max_ctr_complexity', 1, 4),
        'verbose': 0
    }

    # Initialize CatBoost Regressor
    model = CatBoostRegressor(**param,cat_features=object_cols_indices)

    # Train the model
    model.fit(X_train_cat, y_train_cat, eval_set=(X_test_cat, y_test_cat), early_stopping_rounds=100, verbose=0)

    # Make predictions
    preds = model.predict(X_test_cat)

    # Calculate mean squared error
    mse = mean_squared_error(y_test_cat, preds)
    return mse

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100)

# Get the best hyperparameters
best_params = study.best_params
print(f'Best hyperparameters: {best_params}')

catboost_3 =  CatBoostRegressor(iterations= 941, learning_rate=0.07304027607653092, depth= 8,
                                l2_leaf_reg=0.000028112047016719393)
catboost_3.fit(X_train, y_train, verbose=False)
y_pred_cat_3 = catboost_3.predict(X_test)

mlflow.log_param("best_param", best_params)
mlflow.log_metric("r2_score", r2_score(y_test_cat, y_pred_cat_3))
mlflow.log_metric("rmse", np.sqrt(mean_squared_error(y_test_cat,y_pred_cat_3)))
mlflow.log_metric("mse", mean_squared_error(y_test_cat, y_pred_cat_3))
mlflow.log_metric("mape", np.sqrt(mean_absolute_percentage_error(y_test_cat,y_pred_cat_3)))
# End an active MLflow run
mlflow.end_run()


# RandomForest-Gridsearch
mlflow.start_run(experiment_id= exp.experiment_id, run_name= "RandomForestRegressor-GridSearch")

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

# Define model
rf = RandomForestRegressor(random_state=42)

# Define hyperparameters grid
param_grid = {
    'n_estimators': [10,20,40,50,100],
    'max_depth': [3,5,7,9]
    }

# Grid search with cross-validation
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

# Best hyperparameters
print("Best hyperparameters:", grid_search.best_params_)
best_model = grid_search.best_estimator_

# Evaluate on test data
y_pred = best_model.predict(X_test)
print("R² Score:", best_model.score(X_test, y_test))

mlflow.log_param("best_param", grid_search.best_params_)
mlflow.log_metric("r2_score", r2_score(y_test, y_pred))
mlflow.log_metric("rmse", np.sqrt(mean_squared_error(y_test,y_pred)))
mlflow.log_metric("mse", mean_squared_error(y_test, y_pred))
mlflow.log_metric("mape", np.sqrt(mean_absolute_percentage_error(y_test,y_pred)))
# End an active MLflow run
mlflow.end_run()

# XGBRegressor - GridSearch
mlflow.start_run(experiment_id= exp.experiment_id, run_name= "XGBRegressor-GridSearch")

# Define model
rf = xgb.XGBRegressor(random_state=42)

# Define hyperparameters grid
param_grid = {
    'n_estimators': [10,20,40,50,100],
    'max_depth': [3,5,7,9]
    }

# Grid search with cross-validation
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

# Best hyperparameters
print("Best hyperparameters:", grid_search.best_params_)
best_model = grid_search.best_estimator_

# Evaluate on test data
y_pred = best_model.predict(X_test)
print("R² Score:", best_model.score(X_test, y_test))

# Metrics
print("R2 score:", r2_score(y_test, y_pred))
print("Mean squared error:", mean_squared_error(y_test, y_pred))
print(f"Root mean squared error:{np.sqrt(mean_squared_error(y_test,y_pred))}")
print("Mean Absolute Pecentage error:",mean_absolute_percentage_error(y_test,y_pred))


mlflow.log_param("best_param", grid_search.best_params_)
mlflow.log_metric("r2_score", r2_score(y_test, y_pred))
mlflow.log_metric("rmse", np.sqrt(mean_squared_error(y_test,y_pred)))
mlflow.log_metric("mse", mean_squared_error(y_test, y_pred))
mlflow.log_metric("mape", np.sqrt(mean_absolute_percentage_error(y_test,y_pred)))
# End an active MLflow run
mlflow.end_run()

"""