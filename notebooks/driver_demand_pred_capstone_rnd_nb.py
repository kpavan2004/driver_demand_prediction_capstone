# -*- coding: utf-8 -*-
"""Driver Demand Pred - Capstone-RND_NB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BflOJJs2tCR8nom_-3dFwwS7Aw1Yuvst
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import mean_squared_error, r2_score,f1_score,recall_score,precision_score,accuracy_score,mean_absolute_percentage_error
from sklearn.preprocessing import StandardScaler, LabelEncoder
import matplotlib.pyplot as plt
import mlflow
mlflow.set_tracking_uri("http://192.168.1.19:5000/")


# Load the dataset
data_raw = pd.read_csv("train.csv")

data_raw.describe().T

data_raw.head()

data_raw.info()

# Shape of dataset
data_raw.shape

def calculate_picked_time(row):
    # Add a day offset if 'Time_Order_picked' is less than 'Time_Orderd'
    if row['Time_Order_picked'] < row['Time_Orderd']:
        return row['Order_Date'] + pd.DateOffset(days=1) + row['Time_Order_picked']
    else:
        return row['Order_Date'] + row['Time_Order_picked']

# Calculate new feature order_prepare_time
def calculate_time_diff(dataframe: pd.DataFrame):

    # print("Inside calculate_time_diff function")

    df = dataframe.copy()
    # print(df.iloc[0].to_dict())
    # Ensure 'Order_Date' is in datetime format
    df['Order_Date'].head()
    df['Order_Date'] = pd.to_datetime(df['Order_Date'], format='%d-%m-%Y') #

    # Convert 'Time_Orderd' and 'Time_Order_picked' to timedelta
    df['Time_Orderd'] = pd.to_timedelta(df['Time_Orderd'])
    df['Time_Order_picked'] = pd.to_timedelta(df['Time_Order_picked'])

    df['Time_Order_picked_formatted'] = df.apply(calculate_picked_time, axis=1)

    df['Time_Ordered_formatted'] = df['Order_Date'] + df['Time_Orderd']

    # Ensure both columns are datetime before performing subtraction
    df['Time_Order_picked_formatted'] = pd.to_datetime(df['Time_Order_picked_formatted'])
    df['Time_Ordered_formatted'] = pd.to_datetime(df['Time_Ordered_formatted'])

    # Calculate the order preparation time in minutes
    df['order_prepare_time'] = (df['Time_Order_picked_formatted'] - df['Time_Ordered_formatted']).dt.total_seconds() / 60

    # Handle null values by filling with the median
    df['order_prepare_time'] = df['order_prepare_time'].fillna(df['order_prepare_time'].median())

    # Drop all the time & date related columns
    df.drop(['Time_Ordered_formatted', 'Time_Order_picked_formatted'], axis=1, inplace=True)
    df["Order_Date"] = dataframe['Order_Date']
    df["Time_Orderd"] = dataframe['Time_Orderd']
    df['Time_Order_picked'] = dataframe['Time_Order_picked']

    return df

# using haversine method
def haversine_distance(loc_list):
    # earth's radius in km
    R = 6371.0

    # convert lat and lon from deg to radians
    lat1,lon1,lat2,lon2 = map(np.radians,loc_list)
    # diff between lat and lon
    d_lat = lat2 - lat1
    d_lon = lon2 - lon1
    # applying haversine formula
    a = np.sin(d_lat / 2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(d_lon / 2)**2
    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))
    d = R * c
    return round(d,2)

def rename_label(dataframe: pd.DataFrame):
    df = dataframe.copy()
    return df

def add_new_features(dataframe: pd.DataFrame):

    df = dataframe.copy()

    df["Order_Date_tmp"] = pd.to_datetime(df["Order_Date"], format='%d-%m-%Y')

    # Add new features
    df["day_of_week"] = df["Order_Date_tmp"].dt.dayofweek # monday = 0 and sunday = 6
    df["is_weekend"] = df["day_of_week"].apply(lambda x:1 if x in [5,6] else 0) # 5 = saturday and 6 = sunday
    df["quarter"] = df["Order_Date_tmp"].dt.quarter
    df['yr'] = df["Order_Date_tmp"].dt.year
    df['mnth'] = df["Order_Date_tmp"].dt.month_name()
    df.drop(['Order_Date_tmp'], axis=1, inplace=True)

    return df

def data_munging(dataframe: pd.DataFrame):
    df = dataframe.copy()

    df["Weatherconditions"] = df["Weatherconditions"].replace("nan", np.nan, regex=True).replace("NaN", np.nan, regex=True)
    df["Road_traffic_density"] = df["Road_traffic_density"].replace("nan", np.nan, regex=True).replace("NaN", np.nan, regex=True)
    df["Festival"] = df["Festival"].replace("nan", np.nan, regex=True).replace("NaN", np.nan, regex=True)
    df["City_area"] = df["City_area"].replace("nan", np.nan, regex=True).replace("NaN", np.nan, regex=True)
    df["Delivery_person_Age"] = df["Delivery_person_Age"].replace("nan", np.nan, regex=True).replace("NaN", np.nan, regex=True)
    df["Delivery_person_Ratings"] = df["Delivery_person_Ratings"].replace("nan", np.nan, regex=True).replace("NaN", np.nan, regex=True)

    df["Delivery_person_Age"] = df["Delivery_person_Age"].fillna(df["Delivery_person_Age"].astype("float").median())
    median_rating = df["Delivery_person_Ratings"].astype("float").median()
    df["Delivery_person_Ratings"] = df["Delivery_person_Ratings"].fillna(median_rating)
    df["Delivery_person_Ratings"] = df["Delivery_person_Ratings"].replace(6, median_rating)

    # For categorical columns use mode as imputation for np.nan
    cols = ["Weatherconditions","Road_traffic_density","City_area","Festival"]
    for col in cols:
        df[col] = df[col].fillna(df[col].mode()[0])

    #Derive new column city from Delivery_person_ID
    df["City"] = df["Delivery_person_ID"].str.split("RES").str[0].astype(str)

    # Removing condition word from the data
    df["Weatherconditions"] = df["Weatherconditions"].str.split(" ").str[-1]

    return df

def pre_pipeline_preparation(*, data_frame: pd.DataFrame) -> pd.DataFrame:

    # Strip spaces from the object type  columns
    data_frame = data_frame.apply(lambda x: x.str.strip() if x.dtype == "object" else x)

    #Derive order_prepare_time
    data_frame = calculate_time_diff(dataframe=data_frame)

    data_frame = add_new_features(dataframe=data_frame)

    # data_frame = rename_label(dataframe = data_frame )

    #Rename city and Time_taken columns
    data_frame.rename(columns = {"City":"City_area","Time_taken(min)":"Time_taken"},inplace=True)

    #Pre Pipeline data munging
    data_frame = data_munging(dataframe = data_frame)

    #Calcuate the distance between Restaurant and Delivery location
    loc_cols = ["Restaurant_latitude","Restaurant_longitude","Delivery_location_latitude","Delivery_location_longitude"]

    #To fix error for predict.py TypeError: ufunc 'radians' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''
    data_frame['Restaurant_latitude'] = pd.to_numeric(data_frame['Restaurant_latitude'], errors='coerce')
    data_frame['Restaurant_longitude'] = pd.to_numeric(data_frame['Restaurant_longitude'], errors='coerce')
    data_frame['Delivery_location_latitude'] = pd.to_numeric(data_frame['Delivery_location_latitude'], errors='coerce')
    data_frame['Delivery_location_longitude'] = pd.to_numeric(data_frame['Delivery_location_longitude'], errors='coerce')

    distance = []
    for i in range(len(data_frame[loc_cols[0]])):
        location_list = [data_frame[loc_cols[j]][i] for j in range(len(loc_cols))]
        distance.append(haversine_distance(location_list))
    data_frame["Distance"] = distance

    # Issue while doing predction as it expects float
    data_frame[['Restaurant_latitude', 'Restaurant_longitude', 'Delivery_location_latitude', 'Delivery_location_longitude']] = data_frame[['Restaurant_latitude', 'Restaurant_longitude', 'Delivery_location_latitude', 'Delivery_location_longitude']].astype(str)

    data_frame['Time_taken'] = data_frame['Time_taken'].str.replace(r'\(min\)', '', regex=True).astype(float)  # Ensure target variable is float

    # Strip spaces from the object type  columns
    data_frame = data_frame.apply(lambda x: x.str.strip() if x.dtype == "object" else x)

    # Drop unnecessary fields
    for field in unused_fields:
        if field in data_frame.columns:
            data_frame.drop(labels = field, axis=1, inplace=True)

    return data_frame

# To remove the rows with zero lat and long
def drop_zero_lat_long(data):
    dataframe = data[-((data["Restaurant_latitude"]==0.0) & (data["Restaurant_longitude"]==0.0)) ]
    return dataframe

data = drop_zero_lat_long(data_raw)

data.shape

features = ['Delivery_person_Age', 'Delivery_person_Ratings', 'Restaurant_latitude', 'Restaurant_longitude', 'Delivery_location_latitude', 'Delivery_location_longitude', 'Weatherconditions', 'Road_traffic_density', 'Vehicle_condition', 'Type_of_order', 'Type_of_vehicle', 'multiple_deliveries', 'Festival', 'City_area', 'City', 'day_of_week', 'is_weekend', 'quarter', 'yr', 'mnth', 'Distance', 'order_prepare_time']

target = ['Time_taken']

unused_fields = ['ID', 'Order_Date', 'Time_Orderd', 'Delivery_person_ID']

data.columns

def load_dataset(*, file_name: str) -> pd.DataFrame:
    dataframe = pd.read_csv(file_name)
    transformed = pre_pipeline_preparation(data_frame = dataframe)
    return transformed

# Read the data and pass through pre pipeline preparation
prep_data = load_dataset(file_name = 'train.csv')

# prep_data[prep_data['Delivery_person_Age'] == 'NaN'].shape[0]

# prep_data['Delivery_person_Age'].isna().sum()

# prep_data["Delivery_person_Age"].astype("float").median()

# prep_data["Delivery_person_Age"] = prep_data["Delivery_person_Age"].fillna(prep_data["Delivery_person_Age"].astype("float").median())
# median_rating = prep_data["Delivery_person_Ratings"].astype("float").median()
# prep_data["Delivery_person_Ratings"] = prep_data["Delivery_person_Ratings"].fillna(median_rating)
# prep_data["Delivery_person_Ratings"] = prep_data["Delivery_person_Ratings"].replace(6, median_rating)

prep_data.info()

prep_data.head(2)

weather_mappings = {
    "Sunny": 0,
    "Stormy": 1,
    "Sandstorms": 2,
    "Cloudy": 3,
    "Fog": 4,
    "Windy": 5
}

traff_den_mappings = {
    "High": 0,
    "Jam": 1,
    "Low": 2,
    "Medium": 3
}

order_type_mappings = {
    "Snack": 0,
    "Drinks": 1,
    "Buffet": 2,
    "Meal": 3
}

vehicle_mappings = {
    "motorcycle": 0,
    "scooter": 1,
    "electric_scooter": 2,
    "bicycle": 3
}

festival_mappings = {
    "No": 0,
    "Yes": 1
}

city_area_mappings = {
    "Urban": 0,
    "Metropolitian": 1,
    "Semi-Urban": 2
}

city_mappings = {
    "INDO": 0,
    "BANG": 1,
    "COIMB": 2,
    "CHEN": 3,
    "HYD": 4,
    "RANCHI": 5,
    "MYS": 6,
    "DEH": 7,
    "KOC": 8,
    "PUNE": 9,
    "LUDH": 10,
    "KNP": 11,
    "MUM": 12,
    "KOL": 13,
    "JAP": 14,
    "SUR": 15,
    "GOA": 16,
    "AURG": 17,
    "AGR": 18,
    "VAD": 19,
    "ALH": 20,
    "BHP": 21
}

mnth_mappings = {
    "January": 0,
    "February": 1,
    "March": 2,
    "April": 3,
    "May": 4,
    "June": 5,
    "July": 6,
    "August": 7,
    "September": 8,
    "October": 9,
    "November": 10,
    "December": 11
}

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import OneHotEncoder

class Mapper(BaseEstimator, TransformerMixin):
    """
    Ordinal categorical variable mapper:
    Treat column as Ordinal categorical variable, and assign values accordingly
    """

    def __init__(self, variable:str, mappings:dict):

        if not isinstance(variable, str):
            raise ValueError("variable name should be a string")

        self.variable = variable
        self.mappings = mappings

    def fit(self, X: pd.DataFrame, y: pd.Series = None):
        # we need the fit statement to accomodate the sklearn pipeline
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = X.copy()
        X[self.variable] = X[self.variable].map(self.mappings).astype(int)

        return X


class OutlierHandler(BaseEstimator, TransformerMixin):
    """
    Change the outlier values:
        - to upper-bound, if the value is higher than upper-bound, or
        - to lower-bound, if the value is lower than lower-bound respectively.
    """

    def __init__(self, variable:str):

        if not isinstance(variable, str):
            raise ValueError("variable name should be a string")

        self.variable = variable

    def fit(self, X: pd.DataFrame, y: pd.Series = None):
        # we need the fit statement to accomodate the sklearn pipeline
        X = X.copy()
        q1 = X.describe()[self.variable].loc['25%']
        q3 = X.describe()[self.variable].loc['75%']
        iqr = q3 - q1
        self.lower_bound = q1 - (1.5 * iqr)
        self.upper_bound = q3 + (1.5 * iqr)

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = X.copy()

        for i in X.index:
            if X.loc[i, self.variable] > self.upper_bound:
                X.loc[i, self.variable]= self.upper_bound
            if X.loc[i, self.variable] < self.lower_bound:
                X.loc[i, self.variable]= self.lower_bound

        return X


class WeekdayOneHotEncoder(BaseEstimator, TransformerMixin):
    """ One-hot encode weekday column """

    def __init__(self, variable:str):

        if not isinstance(variable, str):
            raise ValueError("variable name should be a string")

        self.variable = variable
        self.encoder = OneHotEncoder(sparse_output=False)

    def fit(self, X: pd.DataFrame, y: pd.Series = None):
        # we need the fit statement to accomodate the sklearn pipeline
        X = X.copy()
        self.encoder.fit(X[[self.variable]])
        # Get encoded feature names
        self.encoded_features_names = self.encoder.get_feature_names_out([self.variable])

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = X.copy()

        encoded_weekdays = self.encoder.transform(X[[self.variable]])
        # Append encoded weekday features to X
        X[self.encoded_features_names] = encoded_weekdays

        # drop 'weekday' column after encoding
        X.drop(self.variable, axis=1, inplace=True)

        return X

prep_data_1 = prep_data.copy()

prep_data_1["Weatherconditions"] = prep_data["Weatherconditions"].map(weather_mappings)
prep_data_1["Road_traffic_density"] = prep_data["Road_traffic_density"].map(traff_den_mappings)
prep_data_1["Type_of_order"] = prep_data["Type_of_order"].map(order_type_mappings)
prep_data_1["Type_of_vehicle"] = prep_data["Type_of_vehicle"].map(vehicle_mappings)
prep_data_1["City_area"] = prep_data["City_area"].map(city_area_mappings)
prep_data_1["Festival"] = prep_data["Festival"].map(festival_mappings)
prep_data_1["mnth"] = prep_data["mnth"].map(mnth_mappings)

encoder = LabelEncoder()
prep_data_1['City'] = encoder.fit_transform(prep_data['City'])

# prep_data_1['Time_Order_picked'] = pd.to_timedelta(prep_data['Time_Order_picked']).dt.total_seconds()

prep_data_1.info()

# # Apply One-Hot Encoding using get_dummies
# prep_data_1 = pd.get_dummies(prep_data, columns=['City'], prefix=['City'])
# prep_data_1.head(2)

# # # Apply One-Hot Encoding using class WeekdayOneHotEncoder
# # Initialize the encoder
# city_encoder = WeekdayOneHotEncoder(variable='City')

# # Fit the encoder
# city_encoder.fit(data)

# # Transform the data
# encoded_data = city_encoder.transform(data)

# # Display the transformed data
# print(encoded_data)

# Convert all object columns to float as XGboost throws error for object datatypes
prep_data_1 = prep_data_1.apply(pd.to_numeric, errors='coerce')

prep_data_1.head()

prep_data_1.info()

x = prep_data_1[features]
y = prep_data_1[target].values.ravel()  # This will convert your column-vector y into a 1D array

# divide train and test
X_train, X_test, y_train, y_test = train_test_split(

        x,     # predictors
        y,       # target
        test_size = 0.2,
        random_state= 42   # set the random seed here for reproducibility,
)

print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

# # Step 1: Split into train + temp (test + validation)
# X_train, X_temp, y_train, y_temp = train_test_split(prep_data_1[features], prep_data_1[target], test_size=0.3, random_state=42)

# # Step 2: Split temp into validation and test
# X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# print(X_train.shape)
# print(y_train.shape)
# print(X_test.shape)
# print(y_test.shape)
# print(X_val.shape)
# print(y_val.shape)

client = mlflow.tracking.MlflowClient()

# mlflow.set_tracking_uri("file:///content/mlruns")

experiment_name = "DeliveryTimePrediction-EDA"

# Create or set the experiment
exp = mlflow.set_experiment(experiment_name)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import GridSearchCV

# RandomForestRegressor
mlflow.start_run(experiment_id= exp.experiment_id, run_name='RandomForestRegressor')

# Define model
rf = RandomForestRegressor(random_state=42)

# Define hyperparameters grid
param_grid = {
    'n_estimators': [10,20,40,50,100],
    'max_depth': [3,5,7,9]
    }

# Grid search with cross-validation
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

# Best hyperparameters
print("Best hyperparameters:", grid_search.best_params_)
best_model = grid_search.best_estimator_

# Evaluate on test data
y_pred = best_model.predict(X_test)
print("R² Score:", best_model.score(X_test, y_test))

mlflow.log_param("n_estimators", grid_search.best_params_['n_estimators'])
mlflow.log_param("max_depth", grid_search.best_params_['max_depth'])
mlflow.log_metric("r2_score", r2_score(y_test, y_pred))
mlflow.log_metric("rmse", np.sqrt(mean_squared_error(y_test,y_pred)))
mlflow.log_metric("mse", mean_squared_error(y_test, y_pred))
mlflow.log_metric("mape", np.sqrt(mean_absolute_percentage_error(y_test,y_pred)))

mlflow.end_run()
    
mlflow.start_run(experiment_id= exp.experiment_id, run_name='XGBRegressor')

# XGBOOST

import xgboost as xgb

# Define model
rf = xgb.XGBRegressor(random_state=42)

# Define hyperparameters grid
param_grid = {
    'n_estimators': [10,20,40,50],
    'max_depth': [5,7,9],
    'learning_rate' : [0.01,0.1],
    'min_child_weight' : [0.8, 1],
    'subsample' : [0.8,0.6]
    }

# Grid search with cross-validation
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=3)
grid_search.fit(X_train, y_train)

# Best hyperparameters
print("Best hyperparameters:", grid_search.best_params_)
best_model = grid_search.best_estimator_

# Evaluate on test data
y_pred = best_model.predict(X_test)
print("R² Score:", best_model.score(X_test, y_test))

mlflow.log_param("n_estimators", grid_search.best_params_['n_estimators'])
mlflow.log_param("max_depth", grid_search.best_params_['max_depth'])
mlflow.log_metric("r2_score", r2_score(y_test, y_pred))
mlflow.log_metric("rmse", np.sqrt(mean_squared_error(y_test,y_pred)))
mlflow.log_metric("mse", mean_squared_error(y_test, y_pred))
mlflow.log_metric("mape", np.sqrt(mean_absolute_percentage_error(y_test,y_pred)))

mlflow.end_run()


import optuna
import xgboost as xgb
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

mlflow.start_run(experiment_id= exp.experiment_id, run_name='XGBRegressor-Optuna')
# Load your dataset
# X, y = load_your_data()  # Replace this with your data loading code
X_train, X_test, y_train, y_test = train_test_split( prep_data_1[features],  prep_data_1[target], test_size=0.2, random_state=42)

# Objective function for Optuna hyperparameter tuning
def objective(trial):
    # Hyperparameter search space
    param = {
        'objective': 'reg:squarederror',  # Regression task (RMSE)
        'eval_metric': 'rmse',
        'max_depth': trial.suggest_int('max_depth', 3, 12),  # Depth of tree
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),  # Step size
        'n_estimators': trial.suggest_int('n_estimators', 10, 100),  # Number of boosting rounds
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),  # Fraction of data to use for each tree
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),  # Fraction of features for each tree
        'gamma': trial.suggest_float('gamma', 0, 5),  # Regularization parameter
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),  # Minimum sum of instance weight
    }

    # Train the XGBoost model with the suggested hyperparameters
    model = xgb.XGBRegressor(**param)
    model.fit(X_train, y_train, verbose=2)

    # Get the best RMSE score
    preds = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test,y_pred))(y_test, preds, squared=False)

    return rmse

# Create an Optuna study
study = optuna.create_study(direction='minimize')  # Minimize RMSE

# Optimize the objective function
study.optimize(objective, n_trials=20)  # Run 50 trials

# Get the best hyperparameters
best_params = study.best_trial.params
print(f"Best hyperparameters: {best_params}")

mlflow.log_param("n_estimators", best_params['n_estimators'])
mlflow.log_param("max_depth", best_params['max_depth'])
mlflow.log_metric("r2_score", r2_score(y_test, y_pred))
mlflow.log_metric("rmse", np.sqrt(mean_squared_error(y_test,y_pred)))
mlflow.log_metric("mse", mean_squared_error(y_test, y_pred))
mlflow.log_metric("mape", np.sqrt(mean_absolute_percentage_error(y_test,y_pred)))

mlflow.end_run()


# RandomForestRegressor

# from sklearn.ensemble import RandomForestRegressor
# from sklearn.model_selection import GridSearchCV
# import xgboost as xgb

# # Define model
# rf = xgb.XGBRegressor(random_state=42)

# # Define hyperparameters grid
# param_grid = {
#     'n_estimators': [10,20,40,50,100],
#     'max_depth': [3,5,7,9]
#     }

# # Grid search with cross-validation
# grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=1)
# grid_search.fit(X_train, y_train)

# # Best hyperparameters
# print("Best hyperparameters:", grid_search.best_params_)
# best_model = grid_search.best_estimator_

# # Evaluate on test data
# y_pred = best_model.predict(X_test)
# print("R² Score:", best_model.score(X_test, y_test))

# import optuna
from catboost import CatBoostRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error,mean_absolute_percentage_error

# Catboost

mlflow.start_run(experiment_id= exp.experiment_id, run_name='CatBoostRegressor-Optuna')

categorical_features=['Weatherconditions','Road_traffic_density','Vehicle_condition','Type_of_order','City_area','City','Festival','mnth','day_of_week','quarter']
# Objective function for Optuna
def objective(trial):
    params = {
        "iterations": trial.suggest_int("iterations", 10, 100),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "depth": trial.suggest_int("depth", 4, 10),
        "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1, 5),
        "bagging_temperature": trial.suggest_float("bagging_temperature", 0, 5),
    }
    model = CatBoostRegressor(**params, cat_features=categorical_features, verbose=0, loss_function="RMSE",task_type='GPU')
    model.fit(X_train, y_train, early_stopping_rounds=10)
    y_pred = model.predict(X_test)
    return mean_squared_error(y_test, y_pred)

# Run Optuna study
study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=10)

# Best hyperparameters
print("Best parameters:", study.best_value)

categorical_features=['Weatherconditions','Road_traffic_density','Vehicle_condition','Type_of_order','City_area','City','Festival','mnth','day_of_week','quarter']

params = {
        "iterations": 10,
        "learning_rate": 0.8,
        "depth": 9,
        "l2_leaf_reg": 2,
        "bagging_temperature": 1,
    }

model = CatBoostRegressor(**params, cat_features=categorical_features, verbose=0, loss_function="RMSE")
model.fit(X_train, y_train, early_stopping_rounds=50)
y_pred = model.predict(X_test)
print("R2 score:", r2_score(y_test, y_pred))
print("Mean squared error:", mean_squared_error(y_test, y_pred))
print(f"Root mean squared error:{np.sqrt(mean_squared_error(y_test,y_pred))}")
print("Mean Absolute Pecentage error:",mean_absolute_percentage_error(y_test,y_pred))

mlflow.log_param("n_estimators", study.best_value['n_estimators'])
mlflow.log_param("max_depth", study.best_value['max_depth'])
mlflow.log_metric("r2_score", r2_score(y_test, y_pred))
mlflow.log_metric("rmse", np.sqrt(mean_squared_error(y_test,y_pred)))
mlflow.log_metric("mse", mean_squared_error(y_test, y_pred))
mlflow.log_metric("mape", np.sqrt(mean_absolute_percentage_error(y_test,y_pred)))

mlflow.end_run()


"""
from sklearn.metrics import mean_squared_error, r2_score, mean_squared_error, mean_absolute_percentage_error

# Get feature importance scores
importance = best_model.feature_importances_

# Display feature importance
for feature_name, score in zip(features, importance):
    print(f"{feature_name}: {score:.4f}")

from xgboost import plot_importance
import matplotlib.pyplot as plt

# Plot feature importance
plot_importance(best_model, importance_type="weight")
plt.show()

xgb_model = xgb.XGBRegressor(
 **best_params
)

print("Best Param:",best_params)

# # Model Development
# xgb_model = xgb.XGBRegressor(
#         n_estimators=10,  # Number of boosting rounds (trees)
#     max_depth=9,       # Maximum depth of the trees
#     random_state=42     # Random seed for reproducibility
# )

# Train the model on training data
xgb_model.fit(X_train, y_train)

# Inference: Predict on the test data
y_pred = xgb_model.predict(X_test)

# Metrics
print("R2 score:", r2_score(y_test, y_pred))
print("Mean squared error:", mean_squared_error(y_test, y_pred))
print(f"Root mean squared error:{np.sqrt(mean_squared_error(y_test,y_pred))}")
print("Mean Absolute Pecentage error:",mean_absolute_percentage_error(y_test,y_pred))
"""